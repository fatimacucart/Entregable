# -*- coding: utf-8 -*-
"""Proyecto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18yig1rvMAtJZXz0GLqV1Aejcr-alpAyp

Grupo formado por:
* Silvia Pla Martínez
* Alejandro Moreno Sapena
* Paola Reguera González
* Fátima Cucart Matoses

En esta práctica se aborda un problema de clasificación de sentimientos a partir de un conjunto de datos de tweets relacionados con marcas, medios y servicios de telecomunicaciones. Para ello, se realiza un proceso de exploración y preprocesamiento del texto que incluye tareas de limpieza y normalización, seguido del entrenamiento y evaluación de un modelo basado en redes neuronales convolucionales (CNN) para la identificación automática del sentimiento.

Primero que todo se importan las librerías, la base de datos y se realiza una inspección general de los mismos.
"""

!pip install tensorflow
!pip install langdetect
!pip install unidecode

import pandas as pd
import numpy as np
import re
import string
from google.colab import drive
from unidecode import unidecode
from langdetect import detect, LangDetectException
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, label_binarize
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    confusion_matrix, roc_curve, auc)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, Dense,
    Conv1D, GlobalMaxPooling1D, Dropout)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
import os, random, numpy as np, tensorflow as tf

drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/EDEM IA/IA_predictiva/Redes neuronales Adrián/Proyecto/full-corpus.csv')

print(df.columns)

print(df.head(5))

# Dimensiones (filas, columnas)
df.shape

# Información general: tipos de datos y nulos
df.info()

# Tipos de datos por columna
df.dtypes

# Número de nulos por columna
df.isnull().sum()

# Porcentaje de nulos
df.isnull().mean() * 100

"""A continuación, se lleva a cabo la limpieza y preparación de los datos, que incluye la normalización del texto mediante el uso de minúsculas, la eliminación de signos de puntuación, así como de caracteres especiales y acentos."""

# signos de puntuación estándar + españoles
chars = string.punctuation + "“”‘’¿¡"

# patrón regex para eliminar puntuación
re_punc = re.compile('[%s]' % re.escape(chars))

# limpieza de la columna de texto (una review por fila)
df['comment_text_clean'] = (
    df['TweetText']
    .fillna('')
    .str.replace(re_punc, '', regex=True)
)

# comprobar resultado
df[['TweetText', 'comment_text_clean']].head()

!pip install unidecode

df['comment_text_clean'] = (
    df['comment_text_clean']
    .fillna('')
    .apply(unidecode)
)

df['comment_text_clean'] = df['comment_text_clean'].str.lower()

df['comment_text_clean']

df['tokens'] = df['comment_text_clean'].str.findall(r'\S+')

"""A simple vista, se puede ver que hay muchos comentarios que incluyen URL. Como no aportan nada de valor, las eliminamos."""

df['comment_text_clean'] = df['comment_text_clean'].str.replace(
    r'http\S+|www\S+', '', regex=True
)

"""En esta parte de código se identifica automáticamente el idioma de cada comentario de nuestro dataset, ignorando aquellos que no contienen texto útil. Luego comprobamos cuántos idiomas se han detectado y si la asignación de idiomas es correcta."""

from langdetect import DetectorFactory
DetectorFactory.seed = 42
def safe_detect(text):
    text = str(text).strip()
    if not re.search(r'[A-Za-zÀ-ÿ]', text):
        return 'unknown'
    try:
        return detect(text)
    except LangDetectException:
        return 'unknown'

df['lang'] = df['comment_text_clean'].apply(safe_detect)
df['lang'].value_counts().head(10)

df['lang'].value_counts()

df_es = df[df['lang'] == 'es']
print(df_es.head(5))

df_fr = df[df['lang'] == 'fr']
print(df_fr.head(5))

df_nl = df[df['lang'] == 'nl']
print(df_fr.head(5))

df = df[df['lang'].isin(['en', 'es'])]

"""Se puede observar que las lenguas con menos comentarios no quedan bien clasificadas debido por la falta de suficientes observaciones. Por este motivo, decidimos trabajar únicamente con inglés y español, que son los idiomas con mayor número de comentarios y presentan una clasificación más fiable.

Ahora se eliminan las palabras vacías más comunes (*stopwords*) de cada comentario según su idioma, dejando solo los términos más relevantes.
"""

nltk.download('stopwords')

# diccionario de stopwords por idioma
stop = {
    'en': set(stopwords.words('english')),
    'es': set(stopwords.words('spanish')),
}

def remove_stopwords(tokens, lang):
    sw = stop.get(lang)
    if sw is None:
        return tokens
    return [w for w in tokens if w not in sw]

df['tokens_clean'] = df.apply(lambda r: remove_stopwords(r['tokens'], r['lang']), axis=1)

"""En el siguiente paso se va a entrenar el modelo. Para ello, se va a dividir el dataset en train (70%), validación (15%) y test(15%), pero antes, vamos a ver con cuántos sentimientos diferentes contamos."""

df['Sentiment'].value_counts()

"""Como el sentimiento "irrelevant" y "neutral" pueden ser confusos, vamos a analizar si expresan el mismo significado."""

df['Sentiment'].unique()

pd.set_option('display.max_colwidth', None)

comentarios_muestra = (
    df[df['Sentiment'].isin(['irrelevant', 'neutral'])]
    .groupby('Sentiment', group_keys=False)
    .sample(n=10,random_state=42)
    [['Sentiment', 'comment_text_clean']]
    .reset_index(drop=True)
)

comentarios_muestra

"""Ambas categorías se diferencian claramente: un comentario neutral es relevante para el tema pero no expresa emoción, por ejemplo “Galaxy Nexus vs iPhone 4s, ¿qué smartphone gana?”. En cambio, un comentario irrelevant no aporta información útil ni sentimiento relacionado, como “Lo primero que abro al navegar es Twitter y luego Facebook jajaja”.

Además, dado que ambas clases son las más representadas (2233 irrelevantes y 890 neutrales), si las uniéramos representarían un volumen muy elevado de observaciones frente a las otras dos clases, lo que aumentaría el desbalance del dataset y con ello, la reducción de la capacidad del modelo.

Ahora sí, vamos a preparar el dataset para el modelo, dividiéndolo primero en conjuntos de entrenamiento, validación y test. Luego, se convierten las etiquetas de sentimiento en formato texto (por ejemplo, neutral, positive) a valores numéricos para poder entrenar el modelo.
"""

print("N filas:", len(df))
print(df["lang"].value_counts().head())

resenyas = df["comment_text_clean"]
etiquetas = df["Sentiment"]

# 1️⃣ Train (70%) y Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    resenyas,
    etiquetas,
    test_size=0.3,
    random_state=42,
    stratify=etiquetas
)

# 2️⃣ Validación (15%) y Test (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp,
    y_temp,
    test_size=0.5,
    random_state=42,
    stratify=y_temp
)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(X_val.shape)
print(y_val.shape)

le = LabelEncoder()

y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

class_names = le.classes_
n_classes = len(class_names)

print("Clases:", list(class_names))
print("n_classes:", n_classes)

"""Ahora se ajustan todas las secuencias a la misma longitud mediante padding o truncado, obteniendo matrices listas para entrenar la red neuronal."""

# Hiperparámetros
MAX_WORDS = 20000  # tamaño máximo del vocabulario
MAX_LEN = 200      # longitud máxima de las secuencias

# Asegurar texto limpio
X_train = pd.Series(X_train).fillna("").astype(str)
X_val   = pd.Series(X_val).fillna("").astype(str)
X_test  = pd.Series(X_test).fillna("").astype(str)

# Tokenizador (se ajusta SOLO con train)
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Texto → secuencias
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq   = tokenizer.texts_to_sequences(X_val)
X_test_seq  = tokenizer.texts_to_sequences(X_test)

# Padding / truncado
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding="post", truncating="post")
X_val_pad   = pad_sequences(X_val_seq,   maxlen=MAX_LEN, padding="post", truncating="post")
X_test_pad  = pad_sequences(X_test_seq,  maxlen=MAX_LEN, padding="post", truncating="post")

print("Train padded:", X_train_pad.shape)
print("Val padded:", X_val_pad.shape)
print("Test padded:", X_test_pad.shape)

"""Una vez ya tenemos todos los datos preparados, se entrena un modelo LSTM. Primero se utiliza una capa *embedding* para representar las palabras como vectores numéricos, seguida de una LSTM. La salida se define con *softmax*, ya que se trata de un problema multiclase, y se emplea *sparse_categorical_crossentrop*y porque las etiquetas están codificadas como enteros. Además, se introduce un *dropout* de 0.3 para  evitar sobreajuste. Seguido de ello, sacamos algunas métricas para ver qué tan bien ha funcionado el modelo.

"""

# -------------------------
# Crear arquitectura LSTM
# -------------------------
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
model = Sequential()

# Definir forma de entrada (para que el modelo se construya bien)
model.add(Input(shape=(MAX_LEN,)))

# Embedding entrenable
model.add(
    Embedding(
        input_dim=MAX_WORDS,
        output_dim=32
    )
)

# LSTM con Dropout
model.add(
    LSTM(
        64,
        dropout=0.3
    )
)

# Capa de salida multiclase
model.add(Dense(n_classes, activation="softmax"))

# -------------------------
# Compilar el modelo
# -------------------------
model.compile(
    optimizer=Adam(),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# Resumen del modelo
model.summary()

# -------------------------
# Entrenar el modelo
# -------------------------
history = model.fit(
    X_train_pad,
    y_train_enc,
    epochs=10,
    batch_size=64,
    validation_data=(X_val_pad, y_val_enc),
    shuffle=False,
    verbose=1
)

# -------------------------
# Evaluar el modelo
# -------------------------
loss, accuracy = model.evaluate(X_test_pad, y_test_enc, verbose=0)

print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# --------------------
# EVALUACIÓN EN TEST (Keras)
# --------------------
test_loss, test_acc = model.evaluate(X_test_pad, y_test_enc, verbose=0)
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy (Keras): {test_acc*100:.2f}%")

# --------------------
# PREDICCIONES
# --------------------
y_pred_proba = model.predict(X_test_pad, verbose=0)
y_pred_enc   = np.argmax(y_pred_proba, axis=1)

# --------------------
# MÉTRICAS (sklearn)
# --------------------
print(f"Accuracy (sklearn): {accuracy_score(y_test_enc, y_pred_enc)*100:.2f}%")
print("F1 macro:", round(f1_score(y_test_enc, y_pred_enc, average="macro"), 4))
print("F1 weighted:", round(f1_score(y_test_enc, y_pred_enc, average="weighted"), 4))

print("\nClassification report:\n")
print(classification_report(y_test_enc, y_pred_enc, target_names=class_names))

# --------------------
# MATRIZ DE CONFUSIÓN
# --------------------
cm = confusion_matrix(y_test_enc, y_pred_enc)
cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)

print("\nConfusion matrix (raw):\n")
print(cm)

print("\nConfusion matrix (labeled):\n")
print(cm_df)

"""Como se puede ver, la matriz de confusión muestra que el modelo tiende a predecir casi todos los ejemplos como neutral, lo que evidencia un fuerte efecto del desbalance entre clases. Aunque el *accuracy* alcanza un 53.81%, este resultado está condicionado por la predicción casi exclusiva de la clase mayoritaria (neutral).

Para corregir este problema, se incorpora la técnica de *class_weight*, que asigna mayor peso a las clases minoritarias y obliga al modelo a aprender a diferenciarlas mejor.

"""

# RECREAR MODELO DESDE CERO
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
model = Sequential()
model.add(Input(shape=(MAX_LEN,)))
model.add(Embedding(MAX_WORDS, 32))
model.add(LSTM(64))
model.add(Dense(n_classes, activation="softmax"))

model.compile(
    optimizer=Adam(),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

classes = np.unique(y_train_enc)

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=y_train_enc
)

class_weight_dict = dict(zip(classes, class_weights))


# ENTRENAR CON CLASS WEIGHTS
history = model.fit(
    X_train_pad,
    y_train_enc,
    validation_data=(X_val_pad, y_val_enc),
    epochs=10,
    batch_size=64,
    class_weight=class_weight_dict,
    shuffle=False,
    verbose=1
)

# --------------------
# PREDICCIONES EN TEST
# --------------------
y_pred_proba = model.predict(X_test_pad, verbose=0)
y_pred_enc   = np.argmax(y_pred_proba, axis=1)

# --------------------
# MATRIZ DE CONFUSIÓN
# --------------------
cm = confusion_matrix(y_test_enc, y_pred_enc)

# Etiquetas con nombres
cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)

print("\nConfusion Matrix (labeled):\n")
print(cm_df)

# --------------------
# REPORT DETALLADO
# --------------------
print("\nClassification report:\n")
print(classification_report(y_test_enc, y_pred_enc, target_names=class_names))

"""La nueva matriz de confusión muestra que el modelo ha colapsado nuevamente, pero esta vez prediciendo todos los ejemplos como *negative*, lo que indica que sigue sin aprender a separar correctamente las clases y está siendo dominado por un patrón trivial.

Ante esta falta de discriminación en la LSTM, vamos probar una arquitectura CNN. Se mantiene el uso de *class_weights* para compensar el desbalance entre clases y se incorpora *early_stopping* para detener el entrenamiento cuando el rendimiento en validación deja de mejorar. Como incorporamos esta técnica de regularización, aumentamos el número de epochs a 20, para que el modelo tenga margen de aprendizaje.


"""

# -------------------------
# 0) (Opcional) Semilla para reproducibilidad
# -------------------------
seed=42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# -------------------------
# 1) Modelo CNN para texto
# -------------------------
model = Sequential()
model.add(Input(shape=(MAX_LEN,)))

model.add(Embedding(input_dim=MAX_WORDS, output_dim=32))

# CNN: aprende patrones locales tipo n-gramas
model.add(Conv1D(filters=128, kernel_size=5, activation="relu"))
model.add(GlobalMaxPooling1D())

model.add(Dropout(0.4))
model.add(Dense(64, activation="relu"))
model.add(Dropout(0.4))

# Salida multiclase
model.add(Dense(n_classes, activation="softmax"))

# -------------------------
# 2) Compilar
# -------------------------
model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# -------------------------
# 3) Class weights (balanced)
# -------------------------
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train_enc),
    y=y_train_enc
)
class_weight_dict = dict(enumerate(class_weights))

print("Class weights:", class_weight_dict)

# -------------------------
# 4) Early Stopping
# -------------------------
early_stop = EarlyStopping(
    monitor="val_loss",
    patience=4,
    restore_best_weights=True
)

# -------------------------
# 5) Entrenar
# -------------------------
history = model.fit(
    X_train_pad,
    y_train_enc,
    validation_data=(X_val_pad, y_val_enc),
    epochs=20,
    batch_size=64,
    class_weight=class_weight_dict,
    callbacks=[early_stop],
    shuffle=False,
    verbose=1
)

# -------------------------
# 6) Evaluación + métricas
# -------------------------
test_loss, test_acc = model.evaluate(X_test_pad, y_test_enc, verbose=0)
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy (Keras): {test_acc*100:.2f}%")

y_pred_proba = model.predict(X_test_pad, verbose=0)
y_pred_enc = np.argmax(y_pred_proba, axis=1)

print(f"Accuracy (sklearn): {accuracy_score(y_test_enc, y_pred_enc)*100:.2f}%")
print("F1 micro:", round(f1_score(y_test_enc, y_pred_enc, average="micro"), 4))
print("F1 macro:", round(f1_score(y_test_enc, y_pred_enc, average="macro"), 4))
print("F1 weighted:", round(f1_score(y_test_enc, y_pred_enc, average="weighted"), 4))

print("\nClassification report:\n")
print(classification_report(y_test_enc, y_pred_enc, target_names=class_names))

# -------------------------
# 7) Matriz de confusión
# -------------------------
cm = confusion_matrix(y_test_enc, y_pred_enc)
cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)

print("\nConfusion Matrix (labeled):\n")
print(cm_df)

"""Los resultados muestran que las CNN ofrece un rendimiento más robusto en este problema multiclase, alcanzando una accuracy del 71% y un F1 macro de 0.67, lo que indica una mejora en el equilibrio entre clases. Además, la matriz de confusión refleja que el modelo ya no colapsa en una sola categoría, sino que distingue correctamente la mayoría de ejemplos irrelevant y neutral, evidenciando una capacidad mayor para separar las distintas clases de sentimiento.

Ahora vamos a visualizar estos resultados mediante gráficos.

"""

# Binarizar etiquetas reales: (n_samples, n_classes)
y_test_bin = label_binarize(y_test_enc, classes=np.arange(n_classes))

plt.figure()
for i, cls in enumerate(class_names):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{cls} (AUC={roc_auc:.2f})")

plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Curvas ROC One-vs-Rest (CNN)")
plt.legend()
plt.tight_layout()
plt.show()

"""Este gráfico muestra las curvas ROC para cada clase. En general, las curvas ROC se sitúan claramente por encima de la línea diagonal, lo que indica que el modelo CNN discrimina correctamente entre las distintas clases y obtiene un rendimiento muy superior al azar. En particular, la clase irrelevant presenta el mejor resultado (AUC = 0.97), seguida de negative (AUC = 0.87), mientras que positive (AUC = 0.84) y neutral (AUC = 0.85) muestran también una capacidad de clasificación muy alta aunque ligeramente inferior."""

# Report en formato diccionario
report = classification_report(
    y_test_enc,
    y_pred_enc,
    target_names=class_names,
    output_dict=True
)

# Convertir a DataFrame
df_report = pd.DataFrame(report).transpose()

# Extraer solo F1 de cada clase
f1_scores = df_report.loc[class_names, "f1-score"]

# Plot
plt.figure(figsize=(7,4))
f1_scores.plot(kind="bar")

plt.title("F1-score por clase (CNN)")
plt.ylabel("F1-score")
plt.ylim(0,1)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""El gráfico muestra que el modelo CNN obtiene un rendimiento desigual según la clase: alcanza valores altos de F1 en los tweets irrelevantes y neutrales, mientras que el desempeño es notablemente menor en los sentimientos negativos y positivos. Esto sugiere dificultades para clasificar correctamente las clases minoritarias o más complejas, aunque en mucho más preciso que con las LSTM.

"""

# -------------------------
# Curva de Loss
# -------------------------
plt.figure(figsize=(7,4))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")

plt.title("Evolución del Loss durante el entrenamiento")
plt.xlabel("Épocas")
plt.ylabel("Loss")
plt.legend()
plt.tight_layout()
plt.show()

"""Este último gráfico muestra que el *train loss* disminuye de forma continua, indicando que el modelo aprende bien sobre los datos de entrenamiento. Sin embargo, el *validation loss* deja de mejorar a partir de la época 4–5 y comienza a aumentar, lo que sugiere un posible sobreajuste. Esto indica que el modelo empieza a memorizar el entrenamiento en lugar de generalizar correctamente (*overfitting*).

En conjunto, los resultados obtenidos con las redes neuronales muestran un rendimiento condicionado por el desbalance de clases, favoreciendo la categoría mayoritaria y dificultando la identificación de las clases menos representadas. Además, la evolución del *loss* sugiere la aparición de sobreajuste, puesto que el modelo mejora en entrenamiento pero no generaliza de la misma forma en validación. No obstante, las arquitecturas CNN presentan un desempeño ligeramente superior que son las LSTM, logrando capturar mejor ciertos patrones en comparación con otros modelos evaluados.
"""